{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:00.666399Z","iopub.execute_input":"2025-11-12T02:02:00.666984Z","iopub.status.idle":"2025-11-12T02:02:00.672129Z","shell.execute_reply.started":"2025-11-12T02:02:00.666955Z","shell.execute_reply":"2025-11-12T02:02:00.671380Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR100\nfrom einops import rearrange\nfrom einops.layers.torch import Rearrange\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:00.673317Z","iopub.execute_input":"2025-11-12T02:02:00.673671Z","iopub.status.idle":"2025-11-12T02:02:00.690577Z","shell.execute_reply.started":"2025-11-12T02:02:00.673648Z","shell.execute_reply":"2025-11-12T02:02:00.689739Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"IMG_SIZE = 32\nBATCH_SIZE = 128 \nNUM_WORKERS = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:00.691202Z","iopub.execute_input":"2025-11-12T02:02:00.691440Z","iopub.status.idle":"2025-11-12T02:02:00.704062Z","shell.execute_reply.started":"2025-11-12T02:02:00.691424Z","shell.execute_reply":"2025-11-12T02:02:00.703318Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n])\n\nval_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n])\n\nNUM_CLASSES = 100 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:00.705554Z","iopub.execute_input":"2025-11-12T02:02:00.706007Z","iopub.status.idle":"2025-11-12T02:02:00.717822Z","shell.execute_reply.started":"2025-11-12T02:02:00.705990Z","shell.execute_reply":"2025-11-12T02:02:00.717134Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"root = \"./data\"\ntrain_ds = CIFAR100(root=root, train=True, download=True, transform=train_transform)\nval_ds = CIFAR100(root=root, train=False, download=True, transform=val_transform)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n\nprint(f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}, Classes: {NUM_CLASSES}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:00.718441Z","iopub.execute_input":"2025-11-12T02:02:00.718644Z","iopub.status.idle":"2025-11-12T02:02:02.561897Z","shell.execute_reply.started":"2025-11-12T02:02:00.718630Z","shell.execute_reply":"2025-11-12T02:02:02.561090Z"}},"outputs":[{"name":"stdout","text":"Train samples: 50000, Val samples: 10000, Classes: 100\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"def get_positional_embeddings(sequence_length, d):\n    result = torch.ones(sequence_length, d)\n    for i in range(sequence_length):\n        for j in range(d):\n            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n    return result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:02.562876Z","iopub.execute_input":"2025-11-12T02:02:02.563236Z","iopub.status.idle":"2025-11-12T02:02:02.567719Z","shell.execute_reply.started":"2025-11-12T02:02:02.563208Z","shell.execute_reply":"2025-11-12T02:02:02.566905Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, in_channels=3, patch_size=16, emb_size=256, img_size=224):\n        super().__init__()\n        self.patch_size = patch_size\n        self.img_size = img_size\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        self.projection = nn.Sequential(\n            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n            Rearrange('b e h w -> b (h w) e'),\n        )\n        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n        self.position_embeddings = nn.Parameter(get_positional_embeddings(self.num_patches + 1, emb_size))\n\n    def forward(self, x):\n        b = x.shape[0]\n        x = self.projection(x)\n        cls_tokens = self.cls_token.expand(b, -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)\n        x = x + self.position_embeddings\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:02.568680Z","iopub.execute_input":"2025-11-12T02:02:02.568934Z","iopub.status.idle":"2025-11-12T02:02:02.580985Z","shell.execute_reply.started":"2025-11-12T02:02:02.568918Z","shell.execute_reply":"2025-11-12T02:02:02.580382Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, emb_size, num_heads, dropout=0.1):\n        super().__init__()\n        self.emb_size = emb_size\n        self.num_heads = num_heads\n        self.head_dim = emb_size // num_heads\n        \n        self.qkv = nn.Linear(emb_size, emb_size * 3, bias=False)\n        self.attn_drop = nn.Dropout(dropout)\n        self.projection = nn.Linear(emb_size, emb_size)\n        self.proj_drop = nn.Dropout(dropout)\n\n    def forward(self, x):\n        b, n, e = x.shape\n        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n        attn = F.softmax(attn, dim=-1)\n        attn = self.attn_drop(attn)\n        \n        out = (attn @ v).transpose(1, 2).reshape(b, n, e)\n        out = self.projection(out)\n        out = self.proj_drop(out)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:02.581611Z","iopub.execute_input":"2025-11-12T02:02:02.581792Z","iopub.status.idle":"2025-11-12T02:02:02.596034Z","shell.execute_reply.started":"2025-11-12T02:02:02.581778Z","shell.execute_reply":"2025-11-12T02:02:02.595384Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, emb_size, expansion=4, dropout=0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(emb_size, expansion * emb_size)\n        self.fc2 = nn.Linear(expansion * emb_size, emb_size)\n        self.gelu = nn.GELU()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.gelu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:02.596739Z","iopub.execute_input":"2025-11-12T02:02:02.596979Z","iopub.status.idle":"2025-11-12T02:02:02.611881Z","shell.execute_reply.started":"2025-11-12T02:02:02.596958Z","shell.execute_reply":"2025-11-12T02:02:02.611151Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, emb_size, num_heads, dropout=0.1):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(emb_size)\n        self.attn = MultiHeadAttention(emb_size, num_heads, dropout)\n        self.ln2 = nn.LayerNorm(emb_size)\n        self.mlp = MLP(emb_size, dropout=dropout)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:02.613840Z","iopub.execute_input":"2025-11-12T02:02:02.614090Z","iopub.status.idle":"2025-11-12T02:02:02.625407Z","shell.execute_reply.started":"2025-11-12T02:02:02.614069Z","shell.execute_reply":"2025-11-12T02:02:02.624734Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"class VisionTransformer(nn.Module):\n    def __init__(self, img_size=224, patch_size=16, num_classes=37, \n                 emb_size=256, num_layers=6, num_heads=8, dropout=0.1):\n        super().__init__()\n        \n        self.patch_embed = PatchEmbedding(\n            patch_size=patch_size,\n            emb_size=emb_size,\n            img_size=img_size\n        )\n        \n        self.transformer = nn.Sequential(*[\n            TransformerBlock(emb_size, num_heads, dropout) for _ in range(num_layers)\n        ])\n        \n        self.ln = nn.LayerNorm(emb_size)\n        self.dropout = nn.Dropout(0.2)\n        self.head = nn.Linear(emb_size, num_classes)\n\n    def forward(self, x):\n        x = self.patch_embed(x)\n        x = self.transformer(x)\n        cls_token = x[:, 0]\n        cls_token = self.ln(cls_token)\n        cls_token = self.dropout(cls_token)\n        logits = self.head(cls_token)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:02.626178Z","iopub.execute_input":"2025-11-12T02:02:02.626424Z","iopub.status.idle":"2025-11-12T02:02:02.638940Z","shell.execute_reply.started":"2025-11-12T02:02:02.626401Z","shell.execute_reply":"2025-11-12T02:02:02.638256Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = VisionTransformer(\n    img_size=32,  \n    patch_size=4,  \n    num_classes=100, \n    emb_size=256,\n    num_layers=6,\n    num_heads=8,\n    dropout=0.15\n).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:02.639592Z","iopub.execute_input":"2025-11-12T02:02:02.639836Z","iopub.status.idle":"2025-11-12T02:02:02.888375Z","shell.execute_reply.started":"2025-11-12T02:02:02.639821Z","shell.execute_reply":"2025-11-12T02:02:02.887496Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:02.889259Z","iopub.execute_input":"2025-11-12T02:02:02.889465Z","iopub.status.idle":"2025-11-12T02:02:02.894305Z","shell.execute_reply.started":"2025-11-12T02:02:02.889449Z","shell.execute_reply":"2025-11-12T02:02:02.893555Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"def train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for inputs, labels in dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        # Check for NaN\n        if torch.isnan(loss):\n            print(\"WARNING: NaN loss detected!\")\n            continue\n            \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        running_loss += loss.item() * inputs.size(0)\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n    return running_loss / total, 100. * correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:02.895044Z","iopub.execute_input":"2025-11-12T02:02:02.895380Z","iopub.status.idle":"2025-11-12T02:02:02.909037Z","shell.execute_reply.started":"2025-11-12T02:02:02.895357Z","shell.execute_reply":"2025-11-12T02:02:02.908408Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"def eval_epoch(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n    return running_loss / total, 100. * correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:02.909775Z","iopub.execute_input":"2025-11-12T02:02:02.910059Z","iopub.status.idle":"2025-11-12T02:02:02.926150Z","shell.execute_reply.started":"2025-11-12T02:02:02.910043Z","shell.execute_reply":"2025-11-12T02:02:02.925491Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*50)\nprint(\"Starting Training\")\nprint(\"=\"*50)\n\nbest_val_acc = 0\npatience = 5\npatience_counter = 0\n\nfor epoch in range(1, 101):\n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n    val_loss, val_acc = eval_epoch(model, val_loader, criterion, device)\n    scheduler.step()\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        patience_counter = 0\n        torch.save(model.state_dict(), 'best_vit_cifar100.pth')\n        marker = \" ✓ NEW BEST\"\n    else:\n        patience_counter += 1\n        marker = \"\"\n    \n    if epoch % 5 == 0 or epoch <= 10:\n        print(f\"Epoch {epoch:3d} | Train: {train_acc:5.2f}% | Val: {val_acc:5.2f}% | \"\n              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}{marker}\")\n    \n    if patience_counter >= patience:\n        print(f\"\\n Early stopping triggered after {epoch} epochs\")\n        print(f\"No improvement for {patience} consecutive epochs\")\n        print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n        break\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Training Complete! Best Val Accuracy: {best_val_acc:.2f}%\")\nprint(f\"{'='*50}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T02:02:02.926879Z","iopub.execute_input":"2025-11-12T02:02:02.927074Z","iopub.status.idle":"2025-11-12T02:28:02.919002Z","shell.execute_reply.started":"2025-11-12T02:02:02.927057Z","shell.execute_reply":"2025-11-12T02:28:02.918118Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nStarting Training\n==================================================\nEpoch   1 | Train:  8.27% | Val: 16.45% | Train Loss: 4.1481 | Val Loss: 3.7151 ✓ NEW BEST\nEpoch   2 | Train: 17.55% | Val: 23.57% | Train Loss: 3.6556 | Val Loss: 3.4059 ✓ NEW BEST\nEpoch   3 | Train: 22.70% | Val: 27.58% | Train Loss: 3.4313 | Val Loss: 3.2179 ✓ NEW BEST\nEpoch   4 | Train: 26.01% | Val: 31.46% | Train Loss: 3.2878 | Val Loss: 3.0772 ✓ NEW BEST\nEpoch   5 | Train: 29.08% | Val: 33.57% | Train Loss: 3.1635 | Val Loss: 2.9912 ✓ NEW BEST\nEpoch   6 | Train: 31.35% | Val: 35.46% | Train Loss: 3.0659 | Val Loss: 2.8928 ✓ NEW BEST\nEpoch   7 | Train: 33.45% | Val: 36.93% | Train Loss: 2.9671 | Val Loss: 2.8355 ✓ NEW BEST\nEpoch   8 | Train: 35.92% | Val: 39.69% | Train Loss: 2.8815 | Val Loss: 2.7544 ✓ NEW BEST\nEpoch   9 | Train: 38.21% | Val: 41.20% | Train Loss: 2.7960 | Val Loss: 2.6830 ✓ NEW BEST\nEpoch  10 | Train: 39.78% | Val: 43.32% | Train Loss: 2.7224 | Val Loss: 2.6080 ✓ NEW BEST\nEpoch  15 | Train: 48.49% | Val: 49.15% | Train Loss: 2.4103 | Val Loss: 2.4271 ✓ NEW BEST\nEpoch  20 | Train: 55.82% | Val: 52.61% | Train Loss: 2.1545 | Val Loss: 2.3033 ✓ NEW BEST\nEpoch  25 | Train: 62.25% | Val: 55.16% | Train Loss: 1.9445 | Val Loss: 2.2523 ✓ NEW BEST\nEpoch  30 | Train: 67.66% | Val: 55.71% | Train Loss: 1.7747 | Val Loss: 2.2606 ✓ NEW BEST\nEpoch  35 | Train: 72.78% | Val: 56.61% | Train Loss: 1.6214 | Val Loss: 2.2639\nEpoch  40 | Train: 77.01% | Val: 57.28% | Train Loss: 1.4992 | Val Loss: 2.2695 ✓ NEW BEST\nEpoch  45 | Train: 80.99% | Val: 57.74% | Train Loss: 1.3930 | Val Loss: 2.2814 ✓ NEW BEST\nEpoch  50 | Train: 84.27% | Val: 58.74% | Train Loss: 1.3068 | Val Loss: 2.2776\n\n Early stopping triggered after 52 epochs\nNo improvement for 5 consecutive epochs\nBest validation accuracy: 58.93%\n\n==================================================\nTraining Complete! Best Val Accuracy: 58.93%\n==================================================\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}